# 图像算法调研
计算机视觉是人工智能的重要领域。一方面，数据的呈现形式通常是RGB三通道图像，像素色彩带给人的直观感觉比自然语言处理的语义特征鲜明得多，另一方面，工业界对计算机视觉算法的广泛需求，横跨了图像分类、目标检测、语义分割和实例分割。从yolo系列到transformer系列，模型的应用、算法的迭代、场景的探寻，都有不小的探究和应用价值。
## 环境搭建
- 环境搭建是一切深度学习的开端，有时候搭建GPU环境要耗费好几天的大好青春(当然自己写算子的大牛除外)。在beginner账号下的venv环境，内置pytorch1.13nightly和cuda11.2，与本机的RTX3090算力相配套。可运行activate激活venv环境
```shell
. ~/projects/venv/bin/actiavte
```
## 安全帽检测
- 目标检测部分拟全部采用yolov5进行算法搭建和部署。yolo系列举足轻重的地位不必多说，yolov5作为工程经验的总结，体量比yolov4小了一半以上，精度(mAP)却雄风依旧在。而且近几年推出了pytorch版本的实现，模型的预训练权重也公开在github上，可谓是造福一方。智能图像整个大的检测模块拟利用官方预训练好的yolov5s进行第一版的迁移学习和部署。这个模型的原始输出为：如果图片或视频中人带着安全帽，则检测类型为hat，如果人没戴安全帽，则检测类型为person。故部署在生产环境中时，如果检测到person,则做相应的报警，表示检测到有工人未佩戴安全帽。本权重文件很小，只有15.6MB，推理速度快，但可能存在误检，先部署上线，看看情况再迭代。项目地址：
[Helmet-Detection-Yolov5-master](https://github.com/FanDady/Helmet-Detection-YoloV5)
- 项目克隆地址为
```shell
~/beginner/Helmet-Detection-YoloV5-master/helmet_yolov5
```
同目录下有yolov5s的预训练权重<font color='red'>helmet.pt</font>,可以直接用于detect，在同目录下命令为:
```shell
python detect.py --weights helmet.pt
```
### 后处理之非极大值抑制(Non-Max_Suppression,NMS)
- 在detect.py中，NMS对每一张图像经过Model后的若干锚框进行后处理，采用IoU_threshold和Confidence_threshold的方法消除重复的、置信度低的锚框，分析这一部分的代码很有必要，因为在实际部署中，Model结构是不变的，后处理部分可能需要进行改动。
    - detect.py中对NMS的实现方法为调用torchvision.ops.nms()；针对各个类别，根据class_confidence排序，并根据各锚框IoU(交并比)进行同类别抑制，最后排序输出。
### 后处理之预警提示
- 还没有完善的预警方案，但可以实现一个简单的预警功能。在plot_one_box()函数中增加判断条件，如果检测到person类，则在图片或视频的左上角打印“please wear helmet”的提示。也可以换成中文提示，需要下载对应的字体库到与plots.py同目录下。
### 模型部署
#### Git使用
现在已经在develop上开启的服务代码库在gitlab上，
```shell
git config -l //config list
git config --system --list // default config
git config --global --list //usually name & email
git add -> git commit -> git push  //upload
git pull -> git reset -> git checkout //download
git init //initialize
git branch +new branch
git checkout -b [new branch]
git push -u origin [new branch]
```
#### 在webrtc上进行部署
- helmet_detect已经部署在yjopenapi的webrtc下，server.py的视频处理类中增加了相应的elif分支，可任意传入一帧图像，返回一帧带检测框的图像，并且如果检测到没戴安全帽的人，则在画面左上角打印预警信息。
- 预训练权重未放在yjopenai库中，其路径为~/projects/detection/helmet.pt，如果权重路径被修改，则只需要在helmet_detct()函数中修改为绝对路径即可
- 对helmet_detect.py进行一下注释，阅读过程中完整的注释在webrtc/helmet_yolov5/detect.py中
```python
import cv2
import torch
import torch.backends.cudnn as cudnn
import numpy as np
from models.experimental import attempt_load
from utils.datasets import LoadStreams, LoadImages
from utils.general import  check_img_size, check_requirements, check_imshow, colorstr, non_max_suppression, \
    apply_classifier, scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path, save_one_box
from utils.plots import colors, plot_one_box
from utils.torch_utils import select_device, load_classifier, time_sync
from utils.augmentations import letterbox

@torch.no_grad()                                             #不开启梯度计算
def helmet_detct(img0,
        weights = '/home/yjserver/projects/yjopenapi/webrtc/demo/helmet_yolov5/helmet.pt', #模型保存路径，一定要写绝对路劲！！！！！！！！！！！
        imgsz = 640,                                         #rescale后模型的大小
        conf_thres = 0.25,                                   #置信度阈值，在NMS中用到了两次，如果检测框有冗余，在不考虑重新训练模型的情况下，则考虑调大此数值
        iou_thres = 0.45,                                    #交并比阈值，一般不需要调整
        max_det = 1000,                                      #图片中检测框最大数量
        device  = '0',                                       #做推理的设备选择，0表示单GPU，cpu表示用CPU
        agnostic_nms = False,                                #不考虑类别的NMS，一般不开启
        augment = False,                                     #图片增强，推理时不开启
        line_thickness = 3,                                  #检测框的线型属性
        half = False,                                        #半精度推理，即从FP32调整到FP16，速度加快，但精度下降
        visualize = False,                                   #图片在模型中各个层的特征输出，推理时关闭
        classes = None,                                      #不考虑类别的NMS，一般不开启
        ):
    device = select_device(device)  #检查设备
    model = attempt_load(weights, map_location=device)  #生成yolov5s模型实例并加载权重
    stride = int(model.stride.max())  #Pretrain模型自身带有的步长属性
    names = model.module.names if hasattr(model, 'module') else model.names  #[persen,hat]对应索引为[0,1]
    if half:
        model.half()  #是否开启半精度
    imgsz = check_img_size(imgsz, s=stride)  #检查图片大小参数，并调整至步长的整数倍
    img = letterbox(img0, imgsz, stride=stride)[0]  #对原始图像进行padding和rescale
    img = img.transpose((2,0,1))[::-1]  #调整通道顺序
    img = np.ascontiguousarray(img)  #在底层行存储上使得图片像素连续
    if device.type != 'cpu':         #如果是GPU,则先用零矩阵刷一遍模型
        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))
    img = torch.from_numpy(img).to(device)  #迁移图像至模型所在的设备
    img = img.half() if half else img.float()  #半精度
    img /= 255.0                               #图片像素归一化
    if len(img.shape) == 3:                    #增加batch维度
        img = img[None]
    pred = model(img, augment=augment, visualize=visualize)[0]  #模型预测，pred是若干个检测框；pred.shape是[batchsize,num_bbox,7]，7是xyxy+conf+cls_conf构成的维度
    pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)  #非极大值抑制操作，返回的是最合理的预测框列表，列表中每一个元素是xyxy+conf+cls组成的1x6列向量，这里的conf已经是去除了条件概率，是一个全概率，包括是否有物体和有物体情况下物体类别概率的相乘
    for i, det in enumerate(pred):     #遍历Pred中的预测框进行后处理
        if len(det):                   #如果经过NMS后仍有bbox
            det[:,:4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round() #将bbox尺寸转回到原图尺寸上
            for *xyxy, conf, cls in reversed(det):      #对bbox的1x6的列向量进行解包
                c = int(cls)                            #c是类别索引
                label =  f'{names[c]} {conf:.2f}'       #label包括类别名称和置信度取值，将输出在图片中
                plot_one_box(xyxy, img0, label=label, color=colors(c, True), line_thickness=line_thickness) #plot_one_box先只支持简单输出，并只支持英文输出，如果想要修改字体的话，可以下载对应的ttc文件，进一步修改cv2.PutText的fontStyle属性即可
    return img0

```
在server.py中，不仅要增加相应的分支，还要兼顾路径问题。因此在import处增加这些代码：
```python
import sys
sys.path.append('./helmet_yolov5')
from helmet_yolov5.helmet_detect import helmet_detct
```
#### 部署中存在问题概览
- 模型checkpoint一定要写绝对路径！！！！！！！！！
- 加载模型过后，仍然无法进行inference,但调试后仍然报错：
```python
AttributeError: 'Upsample' object has no attribute 'recompute_scale_factor'
```
    这也是为什么在beginner环境中可以跑通，但yjserver环境中跑不通的根本所在。一开始的时候，已经在beginner环境下修改了这个bug，但在yjserver下没有修改。
    修改本bug的方法：将/home/yjserver/projects/venv/lib/python3.10/site-packages/torch/nn/modules/upsampling.py", line 154的return Upsampling部分去掉。
    bug出现的原因：yolov5并不适配高版本的pytorch，但低版本的pytorch和cuda又不适配RTX3090，因此只能退而求其次，对源文件进行修改。
    bug的危险性：修改bug直接对Upsampling.py进行了修改，难以预测是否会影响到后续算法部署。
- 输出画面有卡顿延迟
    - yolov5s的参数虽然少，模型虽然小，但仍然需要1s的时间导入模型，因此每一帧图片都导入模型的话，肯定会出现延迟。
    需要改变server.py和helmet_detct.py的逻辑，将权重加载单独提取为一个load函数，提前进行checkpoint加载，在用户使用安全帽检测时，只调用detect函数,函数传入每一帧图片和已经导入参数的model。
    在server.py中改变的逻辑是，在服务开启时将model导入一次，并且以参数的形式送入VideoTransform类中，等服务down掉后，model会销毁。之后的模型部署也按类似的套路，若有更好的方法则更新之。
#### 在钢筋检测部署后，我们舍弃了安全帽检测的预训练模型，重新训练了一个Yolov5s V6.0模型，而直接从github下载的预训练模型是基于Yolov5 V5.0的。两者共同加载到内存的时候发生了错乱，模型中的Detect只能用最先导入的那部分。因此统一模型版本至V6.0，解决了此bug。怀疑此bug出现的原因与torch.load加载方法有关，但现阶段还没找到bug的具体来源。重新部署的安全帽检测模型后处理过程与更换之前无甚分别，只是模型内部进行了优化。
#### 模型精度评价
- 采用precision和recall综合评价一个多类别检测模型，两者分别反应了预测结果中符合真实标签的占比和真实标签中预测正确的占比。两者的计算公式如下：
$$precision=\frac{True\_positive}{True\_positive+False\_positive}$$
$$recall=\frac{True\_positive}{True\_positive+False\_negative}$$
如果想要综合评价precision和recall，可以用调和平均数做一个综合判断。调和平均数的意义是能够“惩罚”更拉胯的评价分数，得到的分数往往比算术平均值要低，目标检测中一般被叫做F1-score:
$$F1\_score=\frac{1}{\frac{1}{precision}+\frac{1}{recall}}$$
## 钢筋数量检测
- 经过对标注文件的检查，发现提供的数据集只有Train的部分可用，这部分的Annotations是完整的，Test的部分每张图片只有一根钢筋被标注了，不能用作训练集进行训练，用于validation也比较鸡肋。
- 采用最新版的yolov5 6.0, 很多层相较于之前的版本有改动，比如backbone新增了SPPF，这充分说明了yolov5还焕发着生机与活力。预训练模型基于coco数据集。
- 单卡3090训练，batchsize可以调整至64，丝滑无比。
- 利用cv2.puttext()在图片左上角简单输出图片中的钢筋数目
- 环境测试：之前部署安全帽检测的yolov5时对Upsampling.py的154行进行了修改，现将钢筋检测最新的Yolov5迁移到beginner环境下进行测试。
    - 需要pip intall Ipython psutil，detect.py可以对cv2.imread输入的图片进行预测。
- 我们拍摄的钢筋视角是在一捆钢筋的一端，数钢筋的端部。这也比较符合常理，因为人工也得这么数。钢筋之间挨得很近，但其实并不重合。这就暗含着我们可以在推测的时候，将IoU_thres调整地高一些。这样做最大的好处是消掉了一个低置信度锚框框住两个或多个钢筋的情况，另外置信度高的正确预测框也不会因此而被擦除。训练所用的图片只有250张，在epoch=200的训练中，很容易发生过拟合，得到梯度的局部最优解。在developer的实际部署过程中，也看到了误检(即False Negative)的现象，甚至把我的衣领和阴影都识别成了钢筋。现在的钢筋检测模型应用场景比较单一，对摄像头的角度和距离要求都比较高，两者与训练集越接近越好。


    <img src='https://pro-developer.oss-cn-shanghai.aliyuncs.com//202/img/1657261750250_rebar.jpg' width="700" alt="rebar" align=center /> <img src='https://pro-developer.oss-cn-shanghai.aliyuncs.com//202/img/1657261790344_count_rebar.jpg' width="700" alt="rebar" align=center />

## 口罩检测
- 数据集已经上传至OPENDATA，传输格式可供YOLO系列直接训练。由于webrtc还没有对口罩检测单独设置功能入口，现已部署的mask-detection分支还不能直接在developer上直接查看此项功能，但可以将server.py中的现有的判断分支作修改，即可用口罩检测功能替代其他以部署的功能。这里以口罩检测代替安全帽检测为例，可以作如下修改
```python
第88行修改  elif self.transforms == "helmet":  -> elif self.transforms == "mask":
第103行修改 elif self.transforms == "mask":  -> elif self.transforms == "helmet":
```
修改后安全帽检测实际就成了口罩检测的功能
## 裂缝检测
- 裂缝检测采用DeepCrack的finetune模型及其权重，这是武大提出的专门做像素级别裂缝二值化全景分割的模型，效果比SegNet,Unet要好，论文中各模型对比如下：

<img src='https://pro-developer.oss-cn-shanghai.aliyuncs.com//202/img/1657767961192_deepcrack-compare3.png' width="700" align=center/>

- 论文中比较trick的是它的架构，采用了Encoder-Decoder的形式，并采取五层Encoder-Decoder的特征融合和侧边损失融合。诚然，每个深度学习模型都很难说清楚架构设计上的数学原理，但从工程经验和感性认知出发，DeepCrack这样设计的好处是兼顾了局部特征和全局特征，可以把裂缝的延拓情况和裂缝的宽度、高细粒度层面的开展结合起来共同训练。模型架构如下：

<img src='https://pro-developer.oss-cn-shanghai.aliyuncs.com//202/img/1657769040901_deepcrack-network.png' width="700" align=center/>

- 裂缝监测同样已经部署，与之前的Canny裂缝检测是相互替代的关系。在server.py中第78行注释掉，第79行不注释，则表明用DeepCrack进行裂缝检测。实际检测中发现的问题是，模型GPU推理的速度在0.06s/frame左右，无法做到对视频流实时检测。其实语义分割的深度学习模型几乎都有这个问题，因为是对像素级别的数据进行分类，就导致模型参数量大、推理耗时长。但如果实际需求下是对单张照片进行裂缝检测和后处理工作，不需要那么强的时效性，语义分割模型还是可以胜任的。
### GRAD-CAM(Gradient Class Activation Mapping)
- GRAD-CAM是一种对feature map各个部分对图片分类结果的可视化展示方法，由CAM的基础上提出而来。CAM是在训练阶段额外延申出的热力图可视化分支，需要在模型训练之前提前定义好可视化的参数层，随模型一起训练，上手难度比较高。而GRAD-CAM无需在训练阶段动刀，只需一点点的求导修改和图幅绘制，就可以在推理时对每一个测试集样本的feature map进行可视化。可以说，GRAD-CAM是一个相见恨晚的可视化工具。可查看此库[Grad-Cam-Pytorch](https://github.com/yizt/Grad-CAM.pytorch)。像素级别的CAM对应的是多层feature map的加权求和，由于DeepCrack输出的是二值化的单层feature map,所以在这个网络结构中并不适用，之后做语义分割时会再度启用这个方法。
## 危险区域检测
- 主要实现功能是检测画面中人体是否进入了预设的危险区域，如果人体检测框的中心点进入危险区域多边形，则采取下一步报警措施。为了保证目标检测模型版本的一致，继续复用了YOLOV5 6.0,提取了VOC2012数据集的人体检测部分进行迁移训练。本功能实现的逻辑是：用户在前端绘制危险区域多边形，像后端传入多边形顶点；当有图像传输时，首先可以在图像上将多边形绘制出来方便用户查看；当画面中检测到人时，会将检测框的中心点提取出来，利用pnpoly算法判断中心点是否在危险区域多边形内部。
## 图片标注
- 人工智能，人工在前，智能在后。现在各种能上线的人工智能算法都是有人工标注数据集的强监督模型，半监督、无监督的模型大部分还停留在学术界，能不能用在工业界，还有待探索。尤其是图像这一领域，不管是分类、分割还是检测，都得有大量数据集支撑。如果数据集不够的话，模型泛化性能会毋庸置疑的差，并且模型精度也成问题。因此，标注工作是非常重要的。针对图片分类的标注是比较简单的，我们只需要建立多个类别文件夹，然后将不同种类的图片拖入想分类的文件夹就可以了，后续的工作就交给搞算法的去处理。对于语义分割和实例分割，可以用主流的Labelme和LabelImg画多边形进行标注。
#### 标注工具
- 选择Labelme进行标注。可以用exe版本的Labelme，不用考虑传入参数，但从pypi或github下载的Labelme，就需要传参打开了。于此同时，我们必须准备一个存放类别名的文件，可以是label_names.txt，此文件的结构为的结构：\_\_ignore__ \_background_ 接着是自定义的各种类别，每一个类别占一行。使用Labelme的方法是，pip install labelme，进入标注文件夹，在anaconda prompt用指令：
```shell
labelme Images --labels label_names.txt --nodata --validatelabel exact --config "{shift_auto_shape_color: -2}"
```
## 语义分割
- 语义分割就是把图片中想提取的物体轮廓给画出来，比如我们对室内的窗门板柱感兴趣，那么我们在标注的时候就把这些物体的轮廓画出来，语义分割模型在训练的时候做逐像素的分类。同时，采用实时性的语义分割网络是必要的，这其实也是一个研究领域。近几年语义分割在模型结构上并没有什么突破，哪怕是有了attention的Buff加成。但经过科研人员的探索和炼丹，也积累了不少的实时语义分割模型。经过调研，我们选用高效率的模型bisenet，由旷视在2019年在ECCV提出。这个模型的结构是简练的，特征提取和特征融合的架构有一种简约的美感，虽然不像很多编码-解码结构的网络那样对称。当然，bisenet的效果也有保证，RTX3090实测下来视频处理速度可达50FPS。针对室内场景的语义分割，我们的数据集放在opendata上，目前正依托项目不断进行扩充。
[项目代码](https://github.com/CoinCheung/BiSeNet)

<img src='https://pro-developer.oss-cn-shanghai.aliyuncs.com/202/img/1658805872689_bisenet.jpg' width="700" align=center/>

## 模型推理加速：torch转tensorrt
- 神经网络模型训练之后，参数就被固定了，我们将这些参数保存下来，当需要推理的时候，就把参数加载到内存中或者gpu的显存中。cpu的推理速度是很慢的，一般都在gpu中部署模型。有时候，我们需要对模型进行加速，尤其是一些大模型，这时候就需要用到tensorrt。tensorrt是nvidia基于自家的显卡推出的神经网络加速SDK，对外闭源（怪不得Linus要友好地竖中指），只提供了一些接口，方便用户自己写算子。我们尝试用tensorrt部署Bisenet模型，具体流程是将Bisenet的pytorch模型转为onnx再转为tensorrt。onnx是主流的各种模型转换的中介，它保留了一个模型最基本的信息，而略去了各个框架所带的不同特征。模型转为tensorrt后，处理单张图片的速度由1.5FPS升至50FPS，但处理视频的速度由50FPS降至10FPS；且tensorrt的fp32推理结果肉眼可见地差，因此仍部署cuda版本到gpuservice中。处理视频慢的原因可能是opencv读视频后，内存显存的交换变慢了，后续可以用多线程读取视频来优化速度。基于这份工作，我们之后可以用onnx格式去适配其他环境如ios等。

## 图像分类
- 虽然分类模型比较基础，但还是要用图像分类网络做一些事情的。这些网络是随着backbone的提出不断被更新的，往往分类网络最被关注的部分就是backbone的部分。我们最好是先进行筛选，对各路层出不穷的网络优中选优，选参数量小的，但分类效果好的网络。在这样的思想指导下，我们挑选了van(vision-attention-network)-S作为第一个实验。这个网络的参数量只有14M，但其在imagenet1k上的精度超过81%。除此之外，我们也将考虑efficentnet-b3和efficientnet-b4,两者的参数量都小于20M，但精度都超过了81%。
