# 图像算法调研
## 环境搭建
- 在beginner账号下的venv环境，内置pytorch1.13nightly和cuda11.2，与本机的RTX3090算力相配套。可运行activate激活venv环境
```shell
. ~/projects/venv/bin/actiavte
```
## 安全帽检测
- 拟利用官方预训练好的yolov5s进行第一版的部署。这个模型的原始输出为：如果图片或视频中人带着安全帽，则检测类型为hat，如果人没戴安全帽，则检测类型为person。故部署在生产环境中时，如果检测到person,则做相应的报警，表示检测到有工人未佩戴安全帽。本权重文件很小，只有15.6MB，推理速度快，但可能存在误检，先部署上线，看看情况再迭代。项目地址：
[Helmet-Detection-Yolov5-master](https://github.com/FanDady/Helmet-Detection-YoloV5)
- 项目克隆地址为
```shell
~/beginner/Helmet-Detection-YoloV5-master/helmet_yolov5
```
同目录下有yolov5s的预训练权重<font color='red'>helmet.pt</font>,可以直接用于detect，在同目录下命令为:
```shell
python detect.py --weights helmet.pt
```
### 后处理之非极大值抑制(Non-Max_Suppression,NMS)
- 在detect.py中，NMS对每一张图像经过Model后的若干锚框进行后处理，采用IoU_threshold和Confidence_threshold的方法消除重复的、置信度低的锚框，分析这一部分的代码很有必要，因为在实际部署中，Model结构是不变的，后处理部分可能需要进行改动。
    - detect.py中对NMS的实现方法为调用torchvision.ops.nms()；针对各个类别，根据class_confidence排序，并根据各锚框IoU(交并比)进行同类别抑制，最后排序输出。
### 后处理之预警提示
- 还没有完善的预警方案，但可以实现一个简单的预警功能。在plot_one_box()函数中增加判断条件，如果检测到person类，则在图片或视频的左上角打印“please wear helmet”的提示。也可以换成中文提示，需要下载对应的字体库到与plots.py同目录下。
### 模型部署
#### Git使用
现在已经在develop上开启的服务代码库在gitlab上，
```shell
git config -l //config list
git config --system --list // default config
git config --global --list //usually name & email
git add -> git commit -> git push  //upload
git pull -> git reset -> git checkout //download
git init //initialize
git branch +new branch
git checkout -b [new branch]
git push -u origin [new branch]
```
#### 在webrtc上进行部署
- helmet_detect已经部署在yjopenapi的webrtc下，server.py的视频处理类中增加了相应的elif分支，可任意传入一帧图像，返回一帧带检测框的图像，并且如果检测到没戴安全帽的人，则在画面左上角打印预警信息。
- 预训练权重未放在yjopenai库中，其路径为~/projects/detection/helmet.pt，如果权重路径被修改，则只需要在helmet_detct()函数中修改为绝对路径即可
- 对helmet_detect.py进行一下注释，阅读过程中完整的注释在webrtc/helmet_yolov5/detect.py中
```python
import cv2
import torch
import torch.backends.cudnn as cudnn
import numpy as np
from models.experimental import attempt_load
from utils.datasets import LoadStreams, LoadImages
from utils.general import  check_img_size, check_requirements, check_imshow, colorstr, non_max_suppression, \
    apply_classifier, scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path, save_one_box
from utils.plots import colors, plot_one_box
from utils.torch_utils import select_device, load_classifier, time_sync
from utils.augmentations import letterbox

@torch.no_grad()                                             #不开启梯度计算
def helmet_detct(img0,
        weights = '/home/yjserver/projects/yjopenapi/webrtc/demo/helmet_yolov5/helmet.pt', #模型保存路径，一定要写绝对路劲！！！！！！！！！！！
        imgsz = 640,                                         #rescale后模型的大小
        conf_thres = 0.25,                                   #置信度阈值，在NMS中用到了两次，如果检测框有冗余，在不考虑重新训练模型的情况下，则考虑调大此数值
        iou_thres = 0.45,                                    #交并比阈值，一般不需要调整
        max_det = 1000,                                      #图片中检测框最大数量
        device  = '0',                                       #做推理的设备选择，0表示单GPU，cpu表示用CPU
        agnostic_nms = False,                                #不考虑类别的NMS，一般不开启
        augment = False,                                     #图片增强，推理时不开启
        line_thickness = 3,                                  #检测框的线型属性
        half = False,                                        #半精度推理，即从FP32调整到FP16，速度加快，但精度下降
        visualize = False,                                   #图片在模型中各个层的特征输出，推理时关闭
        classes = None,                                      #不考虑类别的NMS，一般不开启
        ):
    device = select_device(device)  #检查设备
    model = attempt_load(weights, map_location=device)  #生成yolov5s模型实例并加载权重
    stride = int(model.stride.max())  #Pretrain模型自身带有的步长属性
    names = model.module.names if hasattr(model, 'module') else model.names  #[persen,hat]对应索引为[0,1]
    if half:
        model.half()  #是否开启半精度
    imgsz = check_img_size(imgsz, s=stride)  #检查图片大小参数，并调整至步长的整数倍
    img = letterbox(img0, imgsz, stride=stride)[0]  #对原始图像进行padding和rescale
    img = img.transpose((2,0,1))[::-1]  #调整通道顺序
    img = np.ascontiguousarray(img)  #在底层行存储上使得图片像素连续
    if device.type != 'cpu':         #如果是GPU,则先用零矩阵刷一遍模型
        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))
    img = torch.from_numpy(img).to(device)  #迁移图像至模型所在的设备
    img = img.half() if half else img.float()  #半精度
    img /= 255.0                               #图片像素归一化
    if len(img.shape) == 3:                    #增加batch维度
        img = img[None]
    pred = model(img, augment=augment, visualize=visualize)[0]  #模型预测，pred是若干个检测框；pred.shape是[batchsize,num_bbox,7]，7是xyxy+conf+cls_conf构成的维度
    pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)  #非极大值抑制操作，返回的是最合理的预测框列表，列表中每一个元素是xyxy+conf+cls组成的1x6列向量，这里的conf已经是去除了条件概率，是一个全概率，包括是否有物体和有物体情况下物体类别概率的相乘
    for i, det in enumerate(pred):     #遍历Pred中的预测框进行后处理
        if len(det):                   #如果经过NMS后仍有bbox
            det[:,:4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round() #将bbox尺寸转回到原图尺寸上
            for *xyxy, conf, cls in reversed(det):      #对bbox的1x6的列向量进行解包
                c = int(cls)                            #c是类别索引
                label =  f'{names[c]} {conf:.2f}'       #label包括类别名称和置信度取值，将输出在图片中
                plot_one_box(xyxy, img0, label=label, color=colors(c, True), line_thickness=line_thickness) #plot_one_box先只支持简单输出，并只支持英文输出，如果想要修改字体的话，可以下载对应的ttc文件，进一步修改cv2.PutText的fontStyle属性即可
    return img0

```
在server.py中，不仅要增加相应的分支，还要兼顾路径问题。因此在import处增加这些代码：
```python
import sys
sys.path.append('./helmet_yolov5')
from helmet_yolov5.helmet_detect import helmet_detct
```
#### 部署中存在问题概览
- 模型checkpoint一定要写绝对路径！！！！！！！！！
- 加载模型过后，仍然无法进行inference,但调试后仍然报错：
```python
AttributeError: 'Upsample' object has no attribute 'recompute_scale_factor'
```
    这也是为什么在beginner环境中可以跑通，但yjserver环境中跑不通的根本所在。一开始的时候，已经在beginner环境下修改了这个bug，但在yjserver下没有修改修改。
    修改本bug的方法：将/home/yjserver/projects/venv/lib/python3.10/site-packages/torch/nn/modules/upsampling.py", line 154的return Upsampling部分去掉。
    bug出现的原因：yolov5并不适配高版本的pytorch，但低版本的pytorch和cuda又不适配RTX3090，因此只能退而求其次，对源文件进行修改。
    bug的危险性：修改bug直接对Upsampling.py进行了修改，难以预测是否会影响到后续算法部署。
- 输出画面有卡顿延迟
    - yolov5s的参数虽然少，模型虽然小，但仍然需要1s的时间导入模型，因此每一帧图片都导入模型的话，肯定会出现延迟。
    需要改变server.py和helmet_detct.py的逻辑，将权重加载单独提取为一个load函数，提前进行checkpoint加载，在用户使用安全帽检测时，只调用detect函数,函数传入每一帧图片和已经导入参数的model。
    在server.py中改变的逻辑是，在服务开启时将model导入一次，并且以参数的形式送入VideoTransform类中，等服务down掉后，model会销毁。之后的模型部署也按类似的套路，若有更好的方法则更新之。

# 附aiortc技术原理
aiortc, async io real-time communication.   
aiortc = WebRTC + ORTC  
aiortc是建立在Python异步IO框架asyncio上的实时视频流技术。  
如需更近一步了解aiortc,请阅读下面三个文档
[Web Real-Time Communication (WebRTC)](https://webrtc.org/)
[Object Real-Time Communication (ORTC)](https://ortc.org/)
[aiortc](https://aiortc.readthedocs.io/en/latest/)
## WebRTC
### 为什么要用aiortc
webrtc和ortc技术已在主流浏览器里基本被支持。但是他们内部实现通常较为复杂。如果有了这个python版本，我们就能很方便地插入我们自己的OpenCV算法。本工程中，我们接入建筑领域的识别算法如裂纹、钢筋、安全帽等识别。
### RTCPeerConnection
aiortc.RTCPeerConnection 代表了本地和远程电脑之间的一个WebRTC连接。它的属性有  
  connectionState: connected/connecting/closed/failed/new，当这个属性值改变时，会触发   connectionstatechange  
  iceConnectionState: checking/completed/closed/failed/new, 改变触发 iceconnectionstatechange  
  iceGatheringState: complete/gathering/new, 改变触发icegatheringstatechange  
  localDescription: RTCSessionDescription  
  remoteDescription: RTCSessionDescription  
  sctp: RTCSctpTransport, datachannels用的SCTP transport.  
  signalingState: closed/have-local-offer/have-remote-offer/stable
  addIceCandidate(candidate): add a new RTCIceCandidate received from the remote peer.
  addTrack(track): add a MediaStreamTrack be transmitted to the remote peer. RTCRtpSender
  addTransceiver(trackOrKind, direction='sendrecv'): add a new RTCRtpTransceiver.
  close(): terminate the ICE agent, ending ICE processing and streams.
  createAnswer(): create an SDP answer to an offer received from a remote peer during the offer/answer negotiation of a WebRTC connection. RTCSessionDescription.
  createDataChannel(label): with the given label. RTCDataChannel
  createOffer(): create an SDP offer for the purpose of starting a new WebRTC connection to a remote peer. RTCSessionDescription.
  getReceivers(): returns the list of RTCRtpReceiver objects that are currently attached to the connection. [RTCRtpReceiver]
  getSenders(): returns the list of RTCRtpSender objects that are currently attached to the connection. [RTCRtpSender]
  getStats(): returns statistics for the connection. RTCStatsReport.
  getTransceivers(): return the list of RTCRtpTransceiver objects that are currently attached to the connection. [RTCRtpTransceivers]
  setLocalDescription(sessionDescription): change the local description associated with the connection.
  setRemoteDescription(sessionDescription): change the remote description associated with the connection.
### RTCSessionDescription(sdp, type)
aiortc.RTCSessionDescription dictionary describes one end of a connection and how it's configured.
### RTCConfiguration
aiortc.RTCConfiguration dictionary is options for RTCPeerConnection

## ICE (Interactive Connectivity Establishment)
### RTCIceCandidate
aiortc.RTCIceCandidate(component, foundation, ip, port, priority, protocol, type) The RTCIceCandidate interface represents a candidate Interactive Connectivity Establishment(ICE) configuration which may be used to establish an RTCPeerConnection
### RTCIceGatherer
aiortc.RTCIceGatherer The RTCIceGatherer interface gathers local host, server reflexive and relay candidates, as well as enabling the retrieval of local ICE parameters which can be exchanged in signaling.
  state: the current state of the ICE gatherer.
  gather(): gather ICE candidates.
  getDefaultIceServers(): return the list of default RTCIceServer. [RTCIceServer]
  getLocalCandidates(): retrieve the list of valid local candidates associated with the ICE gatherer.
  getLocalParameters(): retrieve the ICE parameters of the ICE gatherer. RTCIceParameters
aiortc.RTCIceTransport(gatherer) The RTCIceTransport interface allows an application access to information about the ICE transport over which packets are sent and received.
  iceGatherer
  role: Either 'controlling' or 'controlled'
  state
  addRemoteCandidate(candidate)
  getRemoteCandidates(): [RTCIceCandidate]
  start(remoteParameters): initiate connectivity checks.
  stop(): irreversibly stop the RTCIceTransport
aiortc.RTCIceParameters 
aiortc.RTCIceServer(urls): The RTCIceServer dictionary defines how to connect to a single STUN or TURN server.
  
## Datagram Transport Layer Security (DTLS)
aiortc.RTCCertificate(key, cert) enables certificates used by an RTCDtlsTransport
  expires
  getFingerprints()
  generateCertificate()
aiortc.RTCDtlsTransport(transport,certificates) 
  state: new,connecting,connected,closed,failed
  transport: RTCIceTransport instance
  getLocalParameters()
  start(remoteParameters): start DTLS transport negotiation with the parameters of the remote DTLS transport.
  stop(): stop and close the DTLS transport
aiortc.RTCDtlsParameters 
aiortc.RTCDtlsFingerprint(algorithm,value)

## Real-time Transport Protocol(RTP)
aiortc.RTCRtpReceiver(kind, transport) interface manages the reception and decoding of data for a MediaStreamTrack.
  track: MediaStreamTrack
  transport: RTCDtlsTransport
  getCapabilities(kind): return the most optimistic view of the system's capabilities for receiving media of the given kind.
  getStats(): RTCStatsReport
  getSynchronizationSources()
  receive(parameters)
  stop()
aiortc.RTCRtpSender(trackOrKind, transport) interface controls MediaStreamTrack encoding and sending.
  track: MediaStreamTrack
  transport: RTCDtlsTransport
  getCapabilities(kind)
  getStats()
  send(parameters)
  stop()
aiortc.RTCRtpTransceiver interface describes a permanent pairing of an RTCRtpSender and an RTCRtpReceiver.
  currentDirection
  direction
  receiver: RTCRtpReceiver
  sender: RTCRtpSender
  setCodecPreferences(codecs)
  stop()
aiortc.RTCRtpSynchronizationSource(timestamp,source)
aiortc.RTCRtpParameters
  codecs: [RTCRtpCodecParameters]
  headerExtensions: [RTCRtpHeaderExtensionParameters]
  rtcp: RTCRtcpParameters
aiortc.RTCRtpCodecParameters
aiortc.RTCRtcpParameters

## Stream Control Transmission Protocol (SCTP)
aiortc.RTCSctpTransport interface includes information relating to Stream Control Transmission Protocl(SCTP) transport.
aiortc.RTCSctpCapabilities

## Data channels
aiortc.RTCDataChannel The RTCDataChannel interface represents a network channel which can be used for bidirectional peer-to-peer transfers of arbitrary data.
  bufferedAmount
  bufferedAmountLowThreshold
  negotiated
  id
  label
  ordered
  maxPacketLifeTime
  maxRetransmits
  protocol
  readyState
  transport
  close()
  send(data)
aiortc.RTCDataChannelParameters 
  label
  maxPacketLifeTime
  maxRetransmits
  ordered
  protocol
  negotiated
  id

## Helpers
### MediaPlayer
aiortc.contrib.media.MediaPlayer A media source that reads audio and/or video from a file.
```python
# Open a video file
player = MediaPlayer('/path/to/some.mp4')
# Open an HTTP stream.
player = MediaPlayer(
    'http://download.tsi.telecom-paristech.fr/'
    'gpac/dataset/dash/uhd/mux_sources/hevcds_720p30_2M.mp4')
# Open webcam on Linux.
player = MediaPlayer('/dev/video0', format='v4l2', options={'video_size':'640x480'})
# Open webcam on OS X.
player = MediaPlayer('default:none', format='avfoundation', options={'video_size':'640x480'})
# Open webcam on Windows.
player = MediaPlayer('video=Integrated Camera', format='dshow', options={'video_size':'640x480'})
```
### MediaRecorder
aiortc.contrib.media.MediaRecorder A media sink that writes audio and/or video to a file.
```python
player = MediaRecorder('/path/to/file.mp4')
# write to a set of images.
player = MediaRecorder('/path/to/file-%3d.png')
```
### MediaBlackhole
aiortc.contrib.media.MediaBlackhole A media sink that consumes and discards all media.
### MediaRelay
aiortc.contrib.meida.MediaRelay A media source that relays one or more tracks to multiple consumers. This is especially useful for live tracks such as webcams or media received over the network.

# STUN和TURN服务器 - ICE框架
为了建立P2P连接，节点需要和另一个节点交换媒体类型，并告诉对方什么时候我要开始或结束通讯。他们需要互相在网络上找到对方。这个过程叫signaling. 我们只关心最后一步，怎么直接建立连接。由于用户设备通常没有公共ip地址。这导致我们需要Interactive Connectivity Establishment (ICE)框架。  
由于IPv4只有大约40亿个地址，而且将被使用殆尽。许多设备只能共用一个public IP并由一个路由器来为他们分发packets. 这个过程叫NAT(Network Address Translation). NAT有很多种，有些会为UDP分配公共IP和端口。这是我们想要的。 所以第一步，查询自己背后的NAT，是这种类型的话，拿到IP地址和端口。
## STUN
Session Traversal Utilities for NAT (STUN)协议可以帮我们做这个。
国内的stun server可以用 stun.qq.com:3478
## TURN
大多数时候，我们需要一个服务器来做P2P通信接力，因为socket直连通常是不可能的(除非在同一局域网)。这就是TURN server (Traversal Using Relay NAT).

## Coturn
我们通过安装Coturn来实现STUN/TURN server.
### 确保端口开放
```
3478 5349 TCP & UDP
49152-65535 UDP
```
### Coturn安装
```shell
sudo apt update
sudo apt install coturn
```
### Enabel coturn
```shell
sudo vim /etc/default/coturn
  TURNSERVER_ENABLED=1
```
### 修改配置文件
```shell
sudo vim /etc/turnserver.conf
```
```shell
listening-port=3478
tls-listening-port=5349

fingerprint
lt-cred-mech

server-name=yijianar.com
realm=yijianar.com

user=guest:somepassword

total-quota=100
stale-nonce=600

cert=/etc/nginx/cert/developer.yijianar.com.pem
pkey=/etc/nginx/cert/developer.yijianar.com.key

cipher-list="ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-SHA384"

proc-user=dev
proc-group=dev
```
```
fingerprint

user=yijian:123456

lt-cred-mech

realm=yijianar.com

log-file=/var/log/turnserver/turnserver.log

simple-log

external-ip=122.9.140.8
```
### 测试
[Trickle Ice](https://webrtc.github.io/samples/src/content/peerconnection/trickle-ice/)
STUN 收到 "srflx"类型的candidate, TURN 收到 "relay"类型的candidate，就说明work了。